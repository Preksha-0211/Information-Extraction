{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsjC8sN-qUa4"
      },
      "source": [
        "### Install and import required packages"
      ],
      "id": "KsjC8sN-qUa4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60uDEtc10LTN"
      },
      "outputs": [],
      "source": [
        "# !pip install keras\n",
        "# !pip install scikit-learn\n",
        "# !pip install transformers\n",
        "# !pip install torch torchvision torchaudio"
      ],
      "id": "60uDEtc10LTN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6339b2cd-dc1d-4504-bb3d-11f2375cc674"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import transformers\n",
        "from transformers import BertTokenizer, BertConfig\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from transformers import BertForTokenClassification, AdamW\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ],
      "id": "6339b2cd-dc1d-4504-bb3d-11f2375cc674"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJlsQfj3oLUW"
      },
      "outputs": [],
      "source": [
        "torch.__version__"
      ],
      "id": "iJlsQfj3oLUW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "921YJhIm0Tzp"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda')\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ],
      "id": "921YJhIm0Tzp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d288dfb7-d29e-4ea2-b26d-4106ae2e36bf"
      },
      "outputs": [],
      "source": [
        "transformers.__version__"
      ],
      "id": "d288dfb7-d29e-4ea2-b26d-4106ae2e36bf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjypuaJMrB6e"
      },
      "source": [
        "### Set-up data"
      ],
      "id": "HjypuaJMrB6e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "090065c1-1595-4092-8448-d2bd38a41a1c"
      },
      "outputs": [],
      "source": [
        "#data = pd.read_csv('New_Data.csv', sep=',')\n",
        "data = pd.read_csv('Classified_data.csv', sep=',')\n",
        "\n",
        "data = data.rename(columns={'text': 'Text', 'ner_tag': 'Tag', 'value': 'Word'})\n",
        "\n",
        "data.head()"
      ],
      "id": "090065c1-1595-4092-8448-d2bd38a41a1c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kEZdR8rA4vP"
      },
      "outputs": [],
      "source": [
        "data.dtypes"
      ],
      "id": "2kEZdR8rA4vP"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEJs_lEQrxco"
      },
      "source": [
        "### Set-up data iterator"
      ],
      "id": "lEJs_lEQrxco"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BW8dHEZ4SF-"
      },
      "source": [
        "The class **`GetSentence`** returns a list of tokenized sentence and its corresponding labels."
      ],
      "id": "7BW8dHEZ4SF-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5659e33-701f-45fb-8a7b-34952c5126a2"
      },
      "outputs": [],
      "source": [
        "class GetSentence(object):\n",
        "    def __init__(self, data):\n",
        "        self.n_sent = 1\n",
        "        self.data = data\n",
        "        self.empty = False\n",
        "        agg = lambda s: [(w, t) for w, t in zip(s['Word'].values.tolist(), s['Tag'].values.tolist())]\n",
        "        self.grouped = self.data.groupby('sentence_number').apply(agg)\n",
        "        self.sentences = [s for s in self.grouped]\n",
        "\n",
        "    def get_next(self):\n",
        "        try:\n",
        "            s = self.grouped['{}'.format(self.n_sent)]\n",
        "            self.n_sent += 1\n",
        "            return s\n",
        "        except:\n",
        "            return None"
      ],
      "id": "d5659e33-701f-45fb-8a7b-34952c5126a2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4e4903ca-a764-4707-b0f9-9e7425c67c2f"
      },
      "outputs": [],
      "source": [
        "getter = GetSentence(data)"
      ],
      "id": "4e4903ca-a764-4707-b0f9-9e7425c67c2f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "816603d0-cc8d-4b72-a995-44a488056659"
      },
      "outputs": [],
      "source": [
        "sentences = [[word[0] for word in sentence] for sentence in getter.sentences]\n",
        "sentences[0]"
      ],
      "id": "816603d0-cc8d-4b72-a995-44a488056659"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d22fdc36-f2c5-498c-bf52-add1dee8fe55"
      },
      "outputs": [],
      "source": [
        "labels = [[s[1] for s in sentence] for sentence in getter.sentences]\n",
        "print(labels[0])"
      ],
      "id": "d22fdc36-f2c5-498c-bf52-add1dee8fe55"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbeLFeTOsXDj"
      },
      "source": [
        "### Set of unique tags and its indices"
      ],
      "id": "TbeLFeTOsXDj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5b1d924-4e5d-4a4d-ae95-dd06a27ccc3b"
      },
      "outputs": [],
      "source": [
        "tag_values = list(set(data['Tag'].values))\n",
        "tag_values.append('PAD')\n",
        "tag2idx = {t: i for i, t in enumerate(tag_values)}"
      ],
      "id": "c5b1d924-4e5d-4a4d-ae95-dd06a27ccc3b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVfYtLBPFFeR"
      },
      "outputs": [],
      "source": [
        "tag_values"
      ],
      "id": "BVfYtLBPFFeR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S645qam-sit2"
      },
      "source": [
        "Save **`tag_values`** as it will be required for later use."
      ],
      "id": "S645qam-sit2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysygt8pbu4SY"
      },
      "outputs": [],
      "source": [
        "t_values = open(\"tag_values.pkl\", \"wb\")\n",
        "pickle.dump(tag_values, t_values)\n",
        "t_values.close()"
      ],
      "id": "ysygt8pbu4SY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQ1g-VO-s-Lf"
      },
      "source": [
        "### Set-up BERT tokenizer from pre-trained **`bert-base-german-cased`**"
      ],
      "id": "pQ1g-VO-s-Lf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "885ae799-c5f1-4edb-9020-b0106fa53eb4"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-german-cased', do_lower_case=False)"
      ],
      "id": "885ae799-c5f1-4edb-9020-b0106fa53eb4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21XtumKDtKXp"
      },
      "source": [
        "As with `tag_values`, we will also require **`tokenizer`** for later use."
      ],
      "id": "21XtumKDtKXp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZExDq5xYBFV"
      },
      "outputs": [],
      "source": [
        "save_tokenizer = open(\"tokenizer.pkl\", \"wb\")\n",
        "pickle.dump(tokenizer, save_tokenizer)\n",
        "save_tokenizer.close()"
      ],
      "id": "gZExDq5xYBFV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Zfk56-zuOiw"
      },
      "source": [
        "Since BERT uses **WordPiece**, we also have to make our sentences to similar format.\n",
        "\n",
        "The following function accepts **`sentences`** and **`labels`**, and iterates through every single one of them.\n",
        "\n",
        "Our **`tokenizer`** is applied to every single word from each sentence of **`sentences`**. While doing this, we have to make each sub-word from word has the same label."
      ],
      "id": "_Zfk56-zuOiw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bb56f8db-617d-42d6-b738-49442b271af2"
      },
      "outputs": [],
      "source": [
        "def tokenize_preserve_labels(sentence, text_labels):\n",
        "    tokenized_sentence = []\n",
        "    labels = []\n",
        "\n",
        "    for word, label in zip(sentence, text_labels):\n",
        "      if not isinstance(word, str):\n",
        "        word = str(word)  # Convert non-string word to string\n",
        "\n",
        "      tokenized_word = tokenizer.tokenize(word)\n",
        "      n_subwords = len(tokenized_word)\n",
        "      tokenized_sentence.extend(tokenized_word)\n",
        "      labels.extend([label] * n_subwords)\n",
        "\n",
        "    return tokenized_sentence, labels"
      ],
      "id": "bb56f8db-617d-42d6-b738-49442b271af2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5c72c90-d085-4210-b7b3-615b7ba3a026"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "tokenized_texts_labels = [tokenize_preserve_labels(sent, labels) for sent, labels in zip(sentences, labels)]"
      ],
      "id": "d5c72c90-d085-4210-b7b3-615b7ba3a026"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6i8uiY4zvsdN"
      },
      "source": [
        "Extract **tokens** and **labels** from **`tokenized_texts_labels`**."
      ],
      "id": "6i8uiY4zvsdN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fcb10a0-3573-4588-8c66-4877c3a4ac75"
      },
      "outputs": [],
      "source": [
        "tokenized_texts = [token_label_pair[0] for token_label_pair in tokenized_texts_labels]\n",
        "labels = [token_label_pair[1] for token_label_pair in tokenized_texts_labels]"
      ],
      "id": "6fcb10a0-3573-4588-8c66-4877c3a4ac75"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybsMuO352NMY"
      },
      "outputs": [],
      "source": [
        "tokenized_texts[4]"
      ],
      "id": "ybsMuO352NMY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OG7bd0TAwJ3B"
      },
      "source": [
        "### Apply padding and generate **`attention_mask`**"
      ],
      "id": "OG7bd0TAwJ3B"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d7822b5-c6d2-430f-af5e-8b0dde24ce67"
      },
      "outputs": [],
      "source": [
        "MAX_LEN = 100\n",
        "BATCH_SIZE = 64"
      ],
      "id": "7d7822b5-c6d2-430f-af5e-8b0dde24ce67"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2ebe348-5d60-4b5b-91e4-1dbe3f990108"
      },
      "outputs": [],
      "source": [
        "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts], maxlen=MAX_LEN, dtype='long', value=0.0, truncating='post', padding='post')"
      ],
      "id": "b2ebe348-5d60-4b5b-91e4-1dbe3f990108"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1a8800f-5260-4f48-b562-64448f1925e1"
      },
      "outputs": [],
      "source": [
        "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels], maxlen=MAX_LEN, value=tag2idx['PAD'], padding='post', dtype='long', truncating='post')"
      ],
      "id": "a1a8800f-5260-4f48-b562-64448f1925e1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnZSBunpQwcV"
      },
      "outputs": [],
      "source": [
        "type(tags)"
      ],
      "id": "gnZSBunpQwcV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8a4196b-d5b9-4422-a304-cc42cd0303c8"
      },
      "outputs": [],
      "source": [
        "attention_mask = [[float(i != 0.0) for i in ii] for ii in input_ids]"
      ],
      "id": "c8a4196b-d5b9-4422-a304-cc42cd0303c8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lh1AVCfH0cRs"
      },
      "source": [
        "### Prepare training and testing data"
      ],
      "id": "Lh1AVCfH0cRs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6JKcK_02Ln-"
      },
      "source": [
        "Split data and attention mask."
      ],
      "id": "H6JKcK_02Ln-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beb85d4d-fc6c-46c1-9528-cf6b510d2dde"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(input_ids, tags, random_state=42, test_size=0.1)\n",
        "tr_mask, val_mask, _, _ = train_test_split(attention_mask, input_ids, random_state=42, test_size=0.1)"
      ],
      "id": "beb85d4d-fc6c-46c1-9528-cf6b510d2dde"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8V5RCQ8IScj3"
      },
      "outputs": [],
      "source": [
        "y_train = torch.cuda.LongTensor(y_train)\n"
      ],
      "id": "8V5RCQ8IScj3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "033526d0-ce2c-450b-a807-bf9112ff7444"
      },
      "outputs": [],
      "source": [
        "X_train = torch.tensor(X_train)\n",
        "X_test  = torch.tensor(X_test)\n",
        "y_train = y_train\n",
        "y_test= torch.tensor(y_test)\n",
        "tr_mask, val_mask = torch.tensor(tr_mask), torch.tensor(val_mask)"
      ],
      "id": "033526d0-ce2c-450b-a807-bf9112ff7444"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zikhaibD2OId"
      },
      "source": [
        "Create data-loaders."
      ],
      "id": "zikhaibD2OId"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94639695-e977-4e7a-820a-0135dfa74c8f"
      },
      "outputs": [],
      "source": [
        "train_data = TensorDataset(X_train, tr_mask, y_train)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "valid_data = TensorDataset(X_test, val_mask, y_test)\n",
        "valid_sampler = SequentialSampler(valid_data)\n",
        "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler,batch_size=BATCH_SIZE)"
      ],
      "id": "94639695-e977-4e7a-820a-0135dfa74c8f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZFHGBn71ndF"
      },
      "source": [
        "### Pull and fine-tune **`bert-base-german-cased`** model"
      ],
      "id": "nZFHGBn71ndF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "da724751-3626-43b3-a979-435040595400"
      },
      "outputs": [],
      "source": [
        "model = BertForTokenClassification.from_pretrained('bert-base-german-cased', num_labels=len(tag2idx), output_attentions=False, output_hidden_states=False)"
      ],
      "id": "da724751-3626-43b3-a979-435040595400"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQEHRctF0xmz"
      },
      "outputs": [],
      "source": [
        "model.cuda();"
      ],
      "id": "PQEHRctF0xmz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1db826e5-d274-41f0-864e-5d983479b9fe"
      },
      "outputs": [],
      "source": [
        "FULL_FINETUNING = True\n",
        "if FULL_FINETUNING:\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = ['bias', 'gamma', 'beta']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.01},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.0}\n",
        "    ]\n",
        "else:\n",
        "    param_optimizer = list(model.classifier.named_parameters)\n",
        "    optimizer_grouped_parameters = [{'params': [p for n, p in param_optimizer]}]"
      ],
      "id": "1db826e5-d274-41f0-864e-5d983479b9fe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ae22876-5c98-4a7e-b548-d8851b25e7e4"
      },
      "outputs": [],
      "source": [
        "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, eps=1e-8)"
      ],
      "id": "7ae22876-5c98-4a7e-b548-d8851b25e7e4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuy0Po8W2cI-"
      },
      "source": [
        "### Training and evaluation"
      ],
      "id": "zuy0Po8W2cI-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKG4_9kF2fIo"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=3, verbose=False, delta=0):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model ...')\n",
        "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
        "        self.val_loss_min = val_loss\n"
      ],
      "id": "DKG4_9kF2fIo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2d8bbff-7dd7-4f76-8e5b-333633ca9eff"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 15\n",
        "MAX_GRAD_NORM = 1.0\n",
        "\n",
        "total_steps = len(train_dataloader) * EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "# Initialize EarlyStopping\n",
        "early_stopping = EarlyStopping(patience=5, verbose=True)"
      ],
      "id": "c2d8bbff-7dd7-4f76-8e5b-333633ca9eff"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9352258c-a388-4f0d-88f7-b1d38404e120"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "loss_values, validation_loss_values = [], []\n",
        "# Initialize lists to store per-epoch loss and accuracy\n",
        "train_losses, val_losses = [], []\n",
        "train_accuracies, val_accuracies = [], []\n",
        "\n",
        "for e in range(EPOCHS):\n",
        "    print(f'- Epoch 0{e+1} -')\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        model.zero_grad()\n",
        "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "        loss = outputs[0]\n",
        "        loss.backward()\n",
        "        total_loss += loss.item()\n",
        "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=MAX_GRAD_NORM)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    print('Average train loss:\\t{:.5f}'.format(avg_train_loss))\n",
        "    loss_values.append(avg_train_loss)\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    predictions, true_labels = [], []\n",
        "\n",
        "    for batch in valid_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "\n",
        "        logits = outputs[1].detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        eval_loss += outputs[0].mean().item()\n",
        "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
        "        true_labels.extend(label_ids)\n",
        "\n",
        "    eval_loss = eval_loss / len(valid_dataloader)\n",
        "    validation_loss_values.append(eval_loss)\n",
        "    print('Validation loss:\\t{:.5f}'.format(eval_loss))\n",
        "    val_accuracies.append(eval_accuracy)\n",
        "\n",
        "    print(f'Epoch {e+1}: Validation Loss: {eval_loss:.5f}, Accuracy: {eval_accuracy:.5f}')\n",
        "\n",
        "    # # Early Stopping call\n",
        "    # early_stopping(eval_loss, model)\n",
        "    # if early_stopping.early_stop:\n",
        "    #   print(\"Early stopping\")\n",
        "    #   break\n",
        "\n",
        "    pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels) for p_i, l_i in zip(p, l) if tag_values[l_i] != 'PAD']\n",
        "    valid_tags = [tag_values[l_i] for l in true_labels for l_i in l if tag_values[l_i] != 'PAD']\n",
        "\n",
        "    print('Validation accuracy:\\t{:.5f}'.format(accuracy_score(pred_tags, valid_tags)))\n",
        "    print('Validation precision:\\t{:.5f}'.format(precision_score(pred_tags, valid_tags, average='micro')))\n",
        "    print('Validation recall:\\t{:.5f}'.format(recall_score(pred_tags, valid_tags, average='micro')))\n",
        "    print('Validation f1-score:\\t{:.5f}\\n'.format(f1_score(pred_tags, valid_tags, average='micro')))"
      ],
      "id": "9352258c-a388-4f0d-88f7-b1d38404e120"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LARGmk0j2oZu"
      },
      "source": [
        "Calculate confusion matrix to identify **TP**, **TN**, **FP**, and **FN**. This is required to calculate **Micro- precision**, **recall**, and **F1-Score**."
      ],
      "id": "LARGmk0j2oZu"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report\n"
      ],
      "metadata": {
        "id": "zBLZwamf5ZRv"
      },
      "id": "zBLZwamf5ZRv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnT6goDTWV9J"
      },
      "outputs": [],
      "source": [
        "tags = list(set(valid_tags))"
      ],
      "id": "qnT6goDTWV9J"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQiHmYpqbDA-"
      },
      "outputs": [],
      "source": [
        "# Print classification report\n",
        "print(classification_report(valid_tags, pred_tags))"
      ],
      "id": "AQiHmYpqbDA-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxmjdsOebzJq"
      },
      "outputs": [],
      "source": [
        "true_labels = valid_tags\n",
        "predicted_labels = pred_tags\n",
        "\n",
        "\n",
        "# Calculate confusion matrix\n",
        "conf_matrix = confusion_matrix(true_labels, predicted_labels, labels=tag_values)\n",
        "\n",
        "# Convert the confusion matrix to a DataFrame for better visualization\n",
        "conf_matrix_df = pd.DataFrame(conf_matrix, index=tag_values, columns=tag_values)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(12, 12))\n",
        "sns.heatmap(conf_matrix_df, annot=True, fmt='g')\n",
        "plt.title('Confusion Matrix for NER Entities')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()"
      ],
      "id": "mxmjdsOebzJq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fh7_VLa33ria"
      },
      "source": [
        "Finally, save our model for later use."
      ],
      "id": "Fh7_VLa33ria"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzNxrAsHyVwI"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"model.pt\")"
      ],
      "id": "fzNxrAsHyVwI"
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyWv_Svi--LG",
        "outputId": "d90ec7cc-405a-4d8c-bb77-29cbc889a901"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-german-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS]: LOCATION_ROUTE\n",
            "A1: LOCATION_STREET\n",
            "zwischen: LOCATION_ROUTE\n",
            "AS: LOCATION_ROUTE\n",
            "Munsbach: LOCATION_CITY\n",
            "AS: LOCATION_STOP\n",
            "Flaxweiler: LOCATION_CITY\n",
            "Verkehrsbehinderung: TRIGGER\n",
            "02: DATE\n",
            ".: DATE\n",
            "12: DATE\n",
            ".: DATE\n",
            "2015: DATE\n",
            "ACL: LOCATION_ROUTE\n",
            "_: LOCATION_ROUTE\n",
            "A1: LOCATION_STREET\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForTokenClassification\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import pickle\n",
        "\n",
        "# Load the tokenizer and the model\n",
        "model_path = \"/content/model.pt\"\n",
        "tokenizer_path = \"/content/tokenizer.pkl\"\n",
        "tag_values_path = \"/content/tag_values.pkl\"\n",
        "\n",
        "with open(tokenizer_path, 'rb') as f:\n",
        "    tokenizer = pickle.load(f)\n",
        "\n",
        "model = BertForTokenClassification.from_pretrained(\n",
        "    'bert-base-german-cased',\n",
        "    num_labels=len(tag2idx),  # tag2idx should be known or loaded as well\n",
        "    output_attentions=False,\n",
        "    output_hidden_states=False\n",
        ")\n",
        "ner = model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with open(tag_values_path, 'rb') as f:\n",
        "    tag_values = pickle.load(f)\n",
        "tag2idx = {t: i for i, t in enumerate(tag_values)}\n",
        "\n",
        "# Prepare the text for entity prediction\n",
        "text = \"A1 zwischen AS Munsbach AS Flaxweiler Verkehrsbehinderung 02.12.2015 ACL_A1.\"\n",
        "tokenized_sentence = tokenizer.encode(text)\n",
        "input_ids = pad_sequences([tokenized_sentence], maxlen=100, dtype=\"long\", value=0.0,\n",
        "                          truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]\n",
        "\n",
        "# Convert to tensors\n",
        "input_ids = torch.tensor(input_ids)\n",
        "attention_masks = torch.tensor(attention_masks)\n",
        "\n",
        "# Predict entities\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids, token_type_ids=None, attention_mask=attention_masks)\n",
        "    logits = outputs[0]\n",
        "\n",
        "# Move logits and labels to CPU\n",
        "logits = logits.detach().cpu().numpy()\n",
        "\n",
        "# Find predicted tags\n",
        "predictions = [list(p) for p in np.argmax(logits, axis=2)]\n",
        "\n",
        "# Convert predictions to tag names\n",
        "predicted_tags = [tag_values[p_i] for p in predictions for p_i in p if tag_values[p_i] != \"PAD\"]\n",
        "\n",
        "# Tokenize the text into words (tokens)\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n",
        "\n",
        "# Align the tokens with their predicted tags\n",
        "final_predictions = []\n",
        "for token, label in zip(tokens, predicted_tags):\n",
        "    if token.startswith(\"##\"):\n",
        "        # Combine subword tokens\n",
        "        final_predictions[-1] = (final_predictions[-1][0] + token[2:], final_predictions[-1][1])\n",
        "    else:\n",
        "        final_predictions.append((token, label))\n",
        "\n",
        "# Print the tokens with their labels\n",
        "for token, label in final_predictions:\n",
        "    print(f\"{token}: {label}\")\n"
      ],
      "id": "oyWv_Svi--LG"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}