{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports for data handling and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "\n",
    "# NLTK for text processing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# sklearn for model evaluation\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# PyTorch for deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Custom utilities (assumed to be in '../src/utils.py')\n",
    "sys.path.append(\"../src\")\n",
    "from utils import *\n",
    "\n",
    "#Set visual style for seaborn\n",
    "sns.set(rc={'figure.figsize':(15,12)})\n",
    "\n",
    "# Load and preprocess datasets\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training dataset\n",
    "data = pd.read_csv('Data.csv', sep=',')\n",
    "data = data.rename(columns={'text': 'Text', 'ner_tag': 'Tag', 'value': 'Word'})\n",
    "print(data.head())\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test dataset\n",
    "test = pd.read_csv('dev.csv', sep=',')\n",
    "test = test.rename(columns={'text': 'Text', 'ner_tag': 'Tag', 'value': 'Word'})\n",
    "print(test.head())\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define custom stopwords for traffic-related text in German\n",
    "traffic_stopwords = {\n",
    "    'Zwischen', 'über', 'unter', 'bis', 'nach', 'vor', 'seit', 'um', 'während', 'zwölf', 'zwei', 'drei', \n",
    "    'vier', 'fünf', 'sechs', 'sieben', 'acht', 'neun', 'zehn', 'elf', 'früher', 'später', 'jetzt', 'heute',\n",
    "    'morgen', 'tag', 'nacht', 'uhr', 'halb', 'viertel', 'lang', 'lange', 'kurz', 'groß', 'klein', 'weit', 'oben', \n",
    "    'unten', 'rechts', 'links', 'nord', 'süd', 'ost', 'west', 'zurück', 'immer', 'manchmal', 'oft', 'selten', 'Wetter', \n",
    "    'Regen', 'Schnee', 'Eis', 'Nebel', 'Wind', 'Sonne', 'warm', 'kalt', 'trocken', 'nass'\n",
    "}\n",
    "\n",
    "# Function to clean text data\n",
    "def clean_text(text, custom_stopwords=set()):\n",
    "    \"\"\"\n",
    "    Clean text by removing URLs, hashtags, @mentions, punctuation, emojis, emoticons,\n",
    "    and words from a custom list of stopwords.\n",
    "    \"\"\"\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)  # Remove URLs\n",
    "    text = re.sub(r'\\@\\w+|\\#','', text)  # Remove hashtags and @mentions\n",
    "    text = re.sub(r'[^\\w\\s\\.]', '', text)  # Remove punctuation (excluding periods)\n",
    "    text = re.sub(r'[^\\w\\s\\.,!?]', '', text)  # Remove emojis\n",
    "    words = text.split()  # Split text into words\n",
    "    words_filtered = [word for word in words if word.lower() not in custom_stopwords]  # Remove custom stopwords\n",
    "    return \" \".join(words_filtered)  # Rejoin words to form the cleaned text\n",
    "\n",
    "# Clean the 'Text' column in the dataset\n",
    "data[\"Text\"] = data[\"Text\"].map(lambda x: clean_text(x, traffic_stopwords) if isinstance(x, str) else x)\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the shape of the dataset\n",
    "print(\"Dataset shape:\", data.shape)\n",
    "\n",
    "# Display the distribution of categorical features\n",
    "print(\"Distribution of 'Tag' values:\")\n",
    "print(data['Tag'].value_counts())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values in each column:\")\n",
    "print(data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of sentences in the dataset: {:,}\".format(data[\"sentence_number\"].nunique()))\n",
    "print(\"Total words in the dataset: {:,}\".format(data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Tag\"].value_counts().plot(kind=\"bar\", figsize=(10,5), color = 'teal');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word per sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = data.groupby(\"sentence_number\")[\"Word\"].agg([\"count\"])\n",
    "word_counts = word_counts.rename(columns={\"count\": \"Word count\"})\n",
    "word_counts.hist(bins=50, figsize=(8,6));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing each sentence in the 'Text' column\n",
    "data['tokenized_text'] = data['Text'].apply(lambda x: word_tokenize(str(x)))\n",
    "\n",
    "# Finding the length of each tokenized sentence\n",
    "data['sentence_length'] = data['tokenized_text'].apply(len)\n",
    "\n",
    "# Finding the longest sentence length\n",
    "longest_sentence_length = data['sentence_length'].max()\n",
    "print(\"Longest sentence length:\", longest_sentence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform descriptive analysis\n",
    "unique_texts = data['Text'].nunique()\n",
    "average_length = data['Text'].apply(lambda x: len(x.split())).mean()\n",
    "\n",
    "# Assess data quality\n",
    "missing_values = data['Text'].isnull().sum()\n",
    "duplicate_entries = data['Text'].duplicated().sum()\n",
    "\n",
    "# Review content by displaying a few random text entries\n",
    "sample_texts = data['Text'].sample(5).values\n",
    "\n",
    "# Visualization with a histogram for sentence length\n",
    "sentence_lengths = data['Text'].apply(lambda x: len(x.split()))\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(sentence_lengths, bins=30, kde=True)\n",
    "plt.title(\"Distribution of Sentence Lengths\")\n",
    "plt.xlabel(\"Sentence Length (Number of Words)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Outputs\n",
    "print(f\"Unique texts: {unique_texts}\")\n",
    "print(f\"Average sentence length: {average_length} words\")\n",
    "print(f\"Missing values in 'text' column: {missing_values}\")\n",
    "print(f\"Duplicate entries in 'text' column: {duplicate_entries}\")\n",
    "print(f\"Random sample of text entries: {sample_texts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization: Splitting each entry into words (unigrams)\n",
    "words = [word for sentence in data['Text'] for word in sentence.split()]\n",
    "\n",
    "# Counting the frequencies of each unigram\n",
    "unigram_counts = Counter(words)\n",
    "\n",
    "# Selecting the top 20 most common unigrams for plotting\n",
    "most_common_unigrams = unigram_counts.most_common(20)\n",
    "unigrams, counts = zip(*most_common_unigrams)\n",
    "\n",
    "# Plotting the unigram frequencies\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=list(unigrams), y=list(counts), color= 'Green')\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Unigrams')\n",
    "plt.ylabel('Counts')\n",
    "plt.title('Top 20 Unigrams in the Dataset')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def summary(item_list, limit=None):\n",
    "    count_dict = dict(Counter(item_list))\n",
    "    count_items = sorted(count_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    print(\"Number of unique items: \", len(count_items))\n",
    "    print(\"Average count: \", round(sum(count_dict.values())/len(count_items), 2),\"\\n\")\n",
    "    \n",
    "    total_items = sum(count_dict.values())\n",
    "    proportions = [(item, count / total_items * 100) for item, count in count_items]\n",
    "\n",
    "    # Plot only up to 'limit' items if a limit is specified, else plot all\n",
    "    if limit is not None:\n",
    "        proportions = proportions[:limit]\n",
    "    \n",
    "    labels, sizes = zip(*proportions)\n",
    "\n",
    "    # Set up the pie chart as a donut chart\n",
    "    plt.figure(figsize=(16, 8))  # Increase figure size\n",
    "    plt.pie(sizes, labels=labels, autopct=lambda p: '{:.1f}%'.format(p), startangle=180)\n",
    "    \n",
    "    # Draw a circle at the center of pie to make it a donut\n",
    "    centre_circle = plt.Circle((0, 0), 0.70, color='black', fc='white', linewidth=0)\n",
    "    fig = plt.gcf()\n",
    "    fig.gca().add_artist(centre_circle)\n",
    "    \n",
    "    plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "summary(data['Tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filter the DataFrame for rows where the 'ner_tag' column is 'TRIGGER'\n",
    "trigger_df = data[data['Tag'] == 'TRIGGER']\n",
    "\n",
    "# Count the occurrences of each unique 'trigger' word or phrase\n",
    "trigger_counts = Counter(trigger_df['Word'])\n",
    "\n",
    "# Select the top 10 (or any number you prefer) most common 'trigger' words or phrases\n",
    "most_common_triggers = trigger_counts.most_common(10)\n",
    "triggers, counts = zip(*most_common_triggers)\n",
    "\n",
    "# Plot the counts of the most frequent 'triggers'\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=list(triggers), y=list(counts))\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Triggers')\n",
    "plt.ylabel('Counts')\n",
    "plt.title('Top Trigger Words in the Dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(trigger_counts)\n",
    "\n",
    "# Display the generated image\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')  # Remove the axis\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analyzing tokens\n",
    "summary(data['Word'], 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
